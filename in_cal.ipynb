{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a4290d",
   "metadata": {
    "id": "67b041fc-2db1-4285-ab9c-3d2afd1dc98f"
   },
   "source": [
    "# InCal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4510515f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0e6eec8-cecf-476a-a013-7a1a4fc54ec5",
    "outputId": "84b54fd2-3de5-4e89-c31d-3dc1fea1aea1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from IPython.display import display, HTML\n",
    "from statsmodels.formula.api import ols\n",
    "from collections import OrderedDict, Counter\n",
    "from jupyter_dash import JupyterDash \n",
    "from dash import html\n",
    "from dash import dcc\n",
    "import itertools\n",
    "from dash import no_update\n",
    "from dash import dash_table\n",
    "import dash\n",
    "from dash.dependencies import Input, Output, State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1a045d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def incal_create_df_incal_format(df, dict_groups):\n",
    "    categories_subjects = flat_list(list(dict_groups.values()))\n",
    "    categories_groups = list(dict_groups.keys())\n",
    "\n",
    "    date_time_level = pd.Series((pd.DatetimeIndex(df['Date_Time_1'])),\n",
    "                                name='Date_Time_1')\n",
    "    subjects_level = pd.Series(pd.Categorical(df['subjectID'],\n",
    "                                              categories=categories_subjects,\n",
    "                                              ordered=True),\n",
    "                               name='subjectID')\n",
    "    group_level = pd.Series(pd.Categorical(df['Group'],\n",
    "                                           categories=categories_groups,\n",
    "                                           ordered=True),\n",
    "                            name='Group')\n",
    "\n",
    "    df = df.drop(columns=['Date_Time_1', 'subjectID', 'Group'])\n",
    "\n",
    "    multi_index_dataframe = pd.concat(\n",
    "        [date_time_level, subjects_level, group_level], axis=1)\n",
    "\n",
    "    return pd.DataFrame(df.values,\n",
    "                        index=pd.MultiIndex.from_frame(multi_index_dataframe),\n",
    "                        columns=df.columns.values.tolist())\n",
    "\n",
    "\n",
    "def get_data(__global_df__, categories_columns_names):\n",
    "    # need to find solution to the use of global var\n",
    "    time_series = __global_df__.reset_index(level=categories_columns_names)\n",
    "    order_categoreis_columns(time_series,\n",
    "                             subjectID=dict_groups.values(),\n",
    "                             Group=dict_groups.keys())\n",
    "    return time_series\n",
    "\n",
    "\n",
    "def get_start_and_end_time(tuple_start_end, dict_time_stamps):\n",
    "    start, end = (str(i) for i in tuple_start_end)\n",
    "    return dict_time_stamps[start], dict_time_stamps[end]\n",
    "\n",
    "\n",
    "def trim_df_datetime(df, start_time, end_time):\n",
    "    return df.loc[start_time:end_time]\n",
    "\n",
    "\n",
    "def remove_data(df, outliers_true, start_time, end_time):\n",
    "    if not outliers_true:\n",
    "        return trim_df_datetime(df, start_time, end_time)\n",
    "    outliers_removed = remove_outliers_mixed_df(df, 'subjectID')\n",
    "    return trim_df_datetime(outliers_removed, start_time, end_time).dropna()\n",
    "\n",
    "\n",
    "#  removing subjects or group\n",
    "def get_values_level(df, number_or_index_name):\n",
    "    return df.index.get_level_values(number_or_index_name)\n",
    "\n",
    "\n",
    "def get_difference_from_list_2(list1, list2):\n",
    "    return list(set(list1) - set(list2))\n",
    "\n",
    "\n",
    "def incal_remove_subjects(df, number_or_index_name, subjects_to_remove):\n",
    "    subjects = get_values_level(df, number_or_index_name)\n",
    "    strs_to_ints = lambda l: [int(x) for x in l]\n",
    "    subjects_to_remove_ints = strs_to_ints(subjects_to_remove)\n",
    "    selected_subjects = get_difference_from_list_2(subjects,\n",
    "                                                   subjects_to_remove_ints)\n",
    "    return df.loc[:, selected_subjects, :]\n",
    "\n",
    "\n",
    "def incal_remove_groups(df, number_or_index_name, groups_to_remove):\n",
    "    subjects = get_values_level(df, number_or_index_name)\n",
    "    selected_group = get_difference_from_list_2(subjects, groups_to_remove)\n",
    "    return df.loc[:, :, selected_group]\n",
    "\n",
    "\n",
    "# removing outliears\n",
    "def sort_data_by_ids(df, column_name):\n",
    "    return df.sort_values(column_name)\n",
    "\n",
    "\n",
    "def flat_list(d_list):\n",
    "    '''\n",
    "    dependencies: itertools\n",
    "    '''\n",
    "    return list(itertools.chain.from_iterable(d_list))\n",
    "\n",
    "\n",
    "def slice_df_for_floats_and_category(df, column_name):\n",
    "    return df.select_dtypes(include=['float64']), df.select_dtypes(\n",
    "        include=['category'])\n",
    "\n",
    "\n",
    "def get_subject_ids(df, column_name):\n",
    "    return df[column_name].unique()\n",
    "\n",
    "\n",
    "def calc_mean_and_std_for_df_by_ids(df, ids_values):\n",
    "    return df.groupby(ids_values).agg([np.mean, np.std])\n",
    "\n",
    "\n",
    "def get_lims_upper_and_lower(df_means_and_stds,\n",
    "                             number_of_ids,\n",
    "                             number_featuers_columns,\n",
    "                             by_sd_of=2):\n",
    "    calcs_shape_values = df_means_and_stds.values.reshape(\n",
    "        number_of_ids, number_featuers_columns, 2)\n",
    "    means = calcs_shape_values[:, :, :1]\n",
    "    stds = calcs_shape_values[:, :, 1:]\n",
    "    upper_lims = means + stds * by_sd_of\n",
    "    lower_lims = means - stds * by_sd_of\n",
    "    return upper_lims, lower_lims\n",
    "\n",
    "\n",
    "def reshpe_vlaues_3d_ndarray(ndarray, axis0_dimensions, axis1_columns,\n",
    "                             axis2_rows):\n",
    "    return ndarray.reshape(axis0_dimensions, axis1_columns, axis2_rows)\n",
    "\n",
    "\n",
    "def select_and_replace_outliers(ndarry_of_features, ndarry_uppers_lims,\n",
    "                                ndarry_lowers_lims):\n",
    "    conditiones = [\n",
    "        ndarry_of_features > ndarry_uppers_lims,\n",
    "        ndarry_of_features < ndarry_lowers_lims\n",
    "    ]\n",
    "    choices = [np.nan, np.nan]\n",
    "    return np.select(conditiones, choices, ndarry_of_features)\n",
    "\n",
    "\n",
    "def back_to_2d_ndarray(ndarry_of_features, axis1, axis2):\n",
    "    return ndarry_of_features.reshape(axis1, axis2)\n",
    "\n",
    "\n",
    "def sort_data_by_index(df):\n",
    "    return df.sort_index()\n",
    "\n",
    "\n",
    "def get_categories_cals_names(df):\n",
    "    return df.index.names[1:]\n",
    "\n",
    "\n",
    "def incal_get_categories_col_from_multiindex(df):\n",
    "    levels_names = get_categories_cals_names(df)\n",
    "    get_values_values_from_index = df.reset_index(level=levels_names)\n",
    "    return get_values_values_from_index[levels_names]\n",
    "\n",
    "\n",
    "def remove_outliers_mixed_df(df):\n",
    "    # sourcery skip: inline-immediately-returned-variable\n",
    "    sorted_df = df.sort_index(level=1)\n",
    "    fetuers, ids = df.values, df.index\n",
    "    df_means_and_stds = calc_mean_and_std_for_df_by_ids(\n",
    "        df,\n",
    "        ids.get_level_values(1).astype('int32'))\n",
    "    number_of_ids = len(ids.levels[1].categories.astype('int32'))\n",
    "    fetuers_columns = df.columns\n",
    "    number_featuers_columns = len(fetuers_columns)\n",
    "    upper_lims, lower_lims = get_lims_upper_and_lower(df_means_and_stds,\n",
    "                                                      number_of_ids,\n",
    "                                                      number_featuers_columns)\n",
    "    dimensions_by_numbers_of_ids_upper_lims = reshpe_vlaues_3d_ndarray(\n",
    "        upper_lims, number_of_ids, 1, number_featuers_columns)\n",
    "    dimensions_by_numbers_of_ids_lower_lims = reshpe_vlaues_3d_ndarray(\n",
    "        lower_lims, number_of_ids, 1, number_featuers_columns)\n",
    "    columns_of_each_id = fetuers.shape[0] // number_of_ids\n",
    "    dimensions_by_numbers_of_ids_values = reshpe_vlaues_3d_ndarray(\n",
    "        fetuers, number_of_ids, columns_of_each_id, number_featuers_columns)\n",
    "    outliers_replaced_to_nan_values_ndarray = select_and_replace_outliers(\n",
    "        dimensions_by_numbers_of_ids_values,\n",
    "        dimensions_by_numbers_of_ids_upper_lims,\n",
    "        dimensions_by_numbers_of_ids_lower_lims)\n",
    "    combien_axis0_and_axis1 = number_of_ids * columns_of_each_id\n",
    "    original_df_shape = back_to_2d_ndarray(\n",
    "        outliers_replaced_to_nan_values_ndarray, combien_axis0_and_axis1,\n",
    "        number_featuers_columns)\n",
    "    df_fetuers_without_outliers = pd.DataFrame(original_df_shape,\n",
    "                                               columns=fetuers_columns,\n",
    "                                               index=ids)\n",
    "    df_without_outliers = pd.concat([df_fetuers_without_outliers], axis=1)\n",
    "    return df_without_outliers\n",
    "\n",
    "\n",
    "# 17.1 ms ± 175 µs per loop (mean ± std. dev. of 5 runs, 100 loops each)\n",
    "\n",
    "\n",
    "def incal_set_multindex(df, list_of_multi_index, drop_current_index=False):\n",
    "    ids_indexed_df = df.reset_index(drop=drop_current_index)\n",
    "    return ids_indexed_df.set_index(list_of_multi_index)\n",
    "\n",
    "\n",
    "def create_category_column(df, categories, ordered=True):\n",
    "    '''\n",
    "    order_categoreis_columns make sure the group and subjects in the right order. This is for,\n",
    "    the statiscal analysis. The groups and the subjects needs to be in order of the expriment design.\n",
    "    In order the anova, ancova and anova with interaction to work properly\n",
    "    \n",
    "    '''\n",
    "    return pd.Categorical(df, categories=categories, ordered=True)\n",
    "\n",
    "\n",
    "def replace_ids_to_group_id(ndarray_ids, groups_names, subjects_within_group):\n",
    "    conditiones = [ndarray_ids == n for n in subjects_within_group]\n",
    "    choices = groups_names\n",
    "    return np.select(conditiones, choices, ndarray_ids)\n",
    "\n",
    "\n",
    "def incal_create_group_column_from_ids(df, ids_column_name, dict_groups):\n",
    "    n_ids_multiple_name = lambda name, n: [name] * len(n)\n",
    "    subjects_vlaues = incal_format[ids_column_name].values\n",
    "    items = dict_groups.items()\n",
    "    groups_names = flat_list(\n",
    "        [n_ids_multiple_name(group, ids) for group, ids in items])\n",
    "    subjects_within_groups = flat_list([ids for ids in dict_groups.values()])\n",
    "    return replace_ids_to_group_id(subjects_vlaues, groups_names,\n",
    "                                   subjects_within_groups)\n",
    "\n",
    "\n",
    "def incal_assemble_group_column_in_df(df, ids_column_name, dict_groups,\n",
    "                                      group_column_name):\n",
    "    values = incal_create_group_column_from_ids(df, ids_column_name,\n",
    "                                                dict_groups)\n",
    "    series = pd.Series(values, copy=False, name=group_column_name)\n",
    "    return concat_dfs([incal_format, series])\n",
    "\n",
    "\n",
    "def get_incal_levels_properties(dict_groups):\n",
    "    date_time_type = 'datetime64[ns]'\n",
    "    order_subjects = flat_list(dict_groups.values())\n",
    "    order_groups = list(dict_groups.keys())\n",
    "    return date_time_type, order_subjects, order_groups\n",
    "\n",
    "\n",
    "def design_incal_levels(idx0, idx1, idx2, date_time_type, order_subjects,\n",
    "                        order_groups):\n",
    "    level_0 = idx0.astype(\n",
    "        date_time_type)  #level 0 convert to type of date time\n",
    "    level_1 = create_category_column(\n",
    "        idx1, order_subjects)  #level 0 convert to type of date time\n",
    "    level_2 = create_category_column(\n",
    "        idx2, order_groups)  #level 0 convert to type of date time\n",
    "    return level_0, level_1, level_2\n",
    "\n",
    "\n",
    "def incal_create_levels(df, dict_groups):\n",
    "    # https://stackoverflow.com/questions/34417970/pandas-convert-index-type-in-multiindex-dataframe\n",
    "    date_time_type, order_subjects, order_groups = get_incal_levels_properties(\n",
    "        dict_groups)\n",
    "    idx = df.index\n",
    "    l0, l1, l2 = design_incal_levels(idx.levels[0], idx.levels[1],\n",
    "                                     idx.levels[2], date_time_type,\n",
    "                                     order_subjects, order_groups)\n",
    "    return df.index.set_levels([l0, l1, l2])\n",
    "\n",
    "\n",
    "# group column and set multiindex format for analysis\n",
    "def create_category_column(df, categories, ordered=True):\n",
    "    '''\n",
    "    order_categoreis_columns make sure the group and subjects in the right order. This is for,\n",
    "    the statiscal analysis. The groups and the subjects needs to be in order of the expriment design.\n",
    "    In order the anova, ancova and anova with interaction to work properly\n",
    "    \n",
    "    '''\n",
    "    return pd.Categorical(df, categories=categories, ordered=True)\n",
    "\n",
    "\n",
    "def replace_ids_to_group_id(ndarray_ids, groups_names, subjects_within_group):\n",
    "    conditiones = [ndarray_ids == str(n) for n in subjects_within_group]\n",
    "    choices = groups_names\n",
    "    return np.select(conditiones, choices, ndarray_ids)\n",
    "\n",
    "\n",
    "def incal_create_group_column_from_ids(df, ids_column_name, dict_groups):\n",
    "    n_ids_multiple_name = lambda name, n: [name] * len(n)\n",
    "    subjects_vlaues = df[ids_column_name].values\n",
    "    items = dict_groups.items()\n",
    "    groups_names = flat_list(\n",
    "        [n_ids_multiple_name(group, ids) for group, ids in items])\n",
    "    subjects_within_groups = flat_list([ids for ids in dict_groups.values()])\n",
    "    return replace_ids_to_group_id(subjects_vlaues, groups_names,\n",
    "                                   subjects_within_groups)\n",
    "\n",
    "\n",
    "def incal_assemble_multi_index_format(df, ids_column_name, dict_groups,\n",
    "                                      group_column_name):\n",
    "\n",
    "    date_time = df.index.to_frame().reset_index(drop=True)\n",
    "\n",
    "    subjects = df[ids_column_name].reset_index(drop=True)\n",
    "\n",
    "    subjects_order = [str(n) for n in flat_list(dict_groups.values())]\n",
    "    cat_subjects = create_category_column(subjects, subjects_order)\n",
    "\n",
    "    groups_values = incal_create_group_column_from_ids(df, ids_column_name,\n",
    "                                                       dict_groups)\n",
    "    groups = pd.Series(groups_values, copy=False, name=group_column_name)\n",
    "    cat_groups = create_category_column(groups, dict_groups.keys())\n",
    "    df = df.drop(columns='subjectID')\n",
    "\n",
    "    frame_datetime_subjects_groups = pd.concat([\n",
    "        date_time,\n",
    "        pd.Series(cat_subjects, copy=False, name=ids_column_name),\n",
    "        pd.Series(cat_groups, copy=False, name=group_column_name)\n",
    "    ],\n",
    "                                               axis=1)\n",
    "    multi_index = pd.MultiIndex.from_frame(frame_datetime_subjects_groups)\n",
    "    return pd.DataFrame(df.values, columns=df.columns, index=multi_index)\n",
    "\n",
    "\n",
    "# removing subjects or group\n",
    "def get_values_level(df, number_or_index_name):\n",
    "    return df.index.get_level_values(number_or_index_name)\n",
    "\n",
    "\n",
    "def get_difference_from_list_2(list1, list2):\n",
    "    return list(set(list1) - set(list2))\n",
    "\n",
    "\n",
    "def incal_remove_subjects(df, number_or_index_name, subjects_to_remove):\n",
    "    subjects = get_values_level(df, number_or_index_name)\n",
    "    strs_to_ints = lambda l: [int(x) for x in l]\n",
    "    subjects_to_remove_ints = strs_to_ints(subjects_to_remove)\n",
    "    selected_subjects = get_difference_from_list_2(subjects,\n",
    "                                                   subjects_to_remove_ints)\n",
    "    return df.loc[:, selected_subjects, :]\n",
    "\n",
    "\n",
    "def incal_remove_group(df, number_or_index_name, groups_to_remove):\n",
    "    subjects = get_values_level(df, number_or_index_name)\n",
    "    selected_group = get_difference_from_list_2(subjects, groups_to_remove)\n",
    "    return df.loc[:, :, (selected_group)]\n",
    "\n",
    "\n",
    "def select_columns_by_metebolic_parm(df, param_name, exclude=False):\n",
    "    if exclude == True:\n",
    "        mask = ~df.columns.str.contains(pat=param_name)\n",
    "        return df.loc[:, mask]\n",
    "    mask = df.columns.str.contains(pat=param_name)\n",
    "    return df.loc[:, mask]\n",
    "\n",
    "\n",
    "def selecting_multi_column_by_part_of_name(df, list_pattern_parm):\n",
    "    return df.filter(regex='|'.join(list_pattern_parm))\n",
    "\n",
    "\n",
    "def multi_columns_by_metabolic_param(df, list_met_param, number):\n",
    "    # https://stackoverflow.com/questions/21285380/find-column-whose-name-contains-a-specific-string\n",
    "    columns_for_calc = df.columns[df.columns.astype(\"string\").str.contains(\n",
    "        pat=\"|\".join(list_met_param))]\n",
    "    df_calc = df[columns_for_calc].apply(lambda x: x * number)\n",
    "    drop_old_columns = df.drop(columns_for_calc, axis=1)\n",
    "    return pd.concat([drop_old_columns, df_calc], axis=1)\n",
    "\n",
    "\n",
    "def loop_func_and_dfs(dfs, func, *args):\n",
    "    return [func(df, *args) for df in dfs]\n",
    "\n",
    "\n",
    "def _get_columns_names_list(df):\n",
    "    return df.columns.values.tolist()\n",
    "\n",
    "\n",
    "def _make_dict_to_replace_names(columns_names_list, pattern_addition_to_parms):\n",
    "    leng = len(columns_names_list)\n",
    "    return {\n",
    "        columns_names_list[i]:\n",
    "        pattern_addition_to_parms + columns_names_list[i]\n",
    "        for i in range(leng)\n",
    "    }\n",
    "\n",
    "\n",
    "def _get_actuals_values(df):\n",
    "    df_actuals_features_calculeted = df.diff()\n",
    "    first_row_df_cumuletive = df.iloc[0:1]\n",
    "    return df_actuals_features_calculeted.fillna(first_row_df_cumuletive)\n",
    "\n",
    "\n",
    "def incal_get_actuals_from_cumuletive(df, columns_pattern,\n",
    "                                      pattern_addition_to_parms):\n",
    "    # get just the cumuletive columns from the original df\n",
    "    df_cumuletive_culumns = select_columns_by_metebolic_parm(\n",
    "        df, columns_pattern)\n",
    "    # get the columns names of the cumuletive columns\n",
    "    columns_names = _get_columns_names_list(df_cumuletive_culumns)\n",
    "    # dict to replace names\n",
    "    dict_new_names = _make_dict_to_replace_names(columns_names,\n",
    "                                                 pattern_addition_to_parms)\n",
    "    # replace the columns names of the actuals culumns\n",
    "    df_actuals_features = df_cumuletive_culumns.rename(columns=dict_new_names)\n",
    "    df_actuals = _get_actuals_values(df_actuals_features)\n",
    "    return pd.concat([df, df_actuals], axis=1).drop(columns_names, axis=1)\n",
    "\n",
    "\n",
    "def incal_calc_cumuletive_values(df, columns_pattern):\n",
    "    select_cols = df.columns.astype(\"string\").str.contains(pat=columns_pattern)\n",
    "    actuals = df.loc[:, select_cols]\n",
    "    actuals_columns_names = actuals.columns.values.tolist()\n",
    "    new_cols_names = [\n",
    "        name.replace(columns_pattern, '') for name in actuals_columns_names\n",
    "    ]\n",
    "    langth = len(actuals_columns_names)\n",
    "    cumuletive = actuals.rename(columns={\n",
    "        actuals_columns_names[i]: new_cols_names[i]\n",
    "        for i in range(langth)\n",
    "    }).cumsum()\n",
    "    return pd.concat([df, cumuletive], axis=1)\n",
    "\n",
    "\n",
    "def incal_set_multindex(df, list_of_multi_index):\n",
    "    ids_indexed_df = df.reset_index()\n",
    "    return ids_indexed_df.set_index(list_of_multi_index)\n",
    "\n",
    "\n",
    "def incal_groupby_then_agg(df, list_to_groupby, agg_func):\n",
    "    groupby = df.groupby(list_to_groupby)\n",
    "    return groupby.agg(agg_func)\n",
    "\n",
    "\n",
    "def incal_resample(df_unstacked_subjects, role_to_resmple_by, agg_func):\n",
    "    # refactoring - > make it more genric function\n",
    "    # https://stackoverflow.com/questions/15799162/resampling-within-a-pandas-multiindex\n",
    "    return incal_groupby_then_agg(df_unstacked_subjects, [\n",
    "        pd.Grouper(level='Date_Time_1', freq=role_to_resmple_by),\n",
    "        pd.Grouper(level='subjectID')\n",
    "    ], agg_func)\n",
    "\n",
    "\n",
    "def _multi_index_df_unstack(df_multi_indexed):\n",
    "    return df_multi_indexed.unstack()\n",
    "\n",
    "\n",
    "def _return_original_stacked_df(df_unstacked_subjects):\n",
    "    return df_unstacked_subjects.stack().reset_index(level=1)\n",
    "\n",
    "\n",
    "def incal_cumsum(df, list_of_multi_index, list_columns_names_to_cumsum):\n",
    "    multi_indexed_df = incal_set_multindex(df, list_of_multi_index)\n",
    "    unstacked_df = _multi_index_df_unstack(multi_indexed_df)\n",
    "    cumsum_columns = unstacked_df[list_columns_names_to_cumsum].cumsum()\n",
    "    cumsum_columns.columns = cumsum_columns.columns.map(\n",
    "        lambda s: (s[0] + '_cumsum', s[1]))\n",
    "    concat_cumsum_columns = pd.concat([unstacked_df, cumsum_columns], axis=1)\n",
    "    return _return_original_stacked_df(concat_cumsum_columns)\n",
    "\n",
    "\n",
    "def _right_sepert_first_underscore(string):\n",
    "    return tuple(string.rsplit(\"_\", 1))\n",
    "\n",
    "\n",
    "def _assemble_multi_index_axis_1_df(df, d_list, axis_1_names=[\"\", \"\"]):\n",
    "    # make a multi index\n",
    "    mul_i_columns = pd.MultiIndex.from_tuples(d_list, names=axis_1_names)\n",
    "    # assemble new dataframe with multi index columns\n",
    "    return pd.DataFrame(df.values, index=df.index, columns=mul_i_columns)\n",
    "    # then stack level 1 to the columns (level 1 -> subjects names e.g. 1 2 3...)\n",
    "\n",
    "\n",
    "def incal_wide_to_long_df(wide_df, col_subj_name='subjectID'):\n",
    "    cols_names = _get_columns_names_list(wide_df)\n",
    "    # sepert feature name from cage number and put it in a tuple together ('allmeters', '1')\n",
    "    l_micolumns = [_right_sepert_first_underscore(col) for col in cols_names]\n",
    "    multi_index_axis_1_df = _assemble_multi_index_axis_1_df(\n",
    "        wide_df, l_micolumns, ['', col_subj_name])\n",
    "    # https://pandas.pydata.org/docs/user_guide/reshaping.html\n",
    "    return multi_index_axis_1_df.stack(level=1)\n",
    "\n",
    "\n",
    "def flatten(lst_in_lst):\n",
    "    lst = []\n",
    "    for l in lst_in_lst:\n",
    "        if type(l) in [list, tuple, set]:\n",
    "            lst.extend(l)\n",
    "        else:\n",
    "            return lst_in_lst\n",
    "    return lst\n",
    "\n",
    "\n",
    "def order_categoreis_columns(df, **kargs):\n",
    "    '''\n",
    "    order_categoreis_columns make sure the group and subjects in the right order. This is for,\n",
    "    the statiscal analysis. The groups and the subjects needs to be in order of the expriment design.\n",
    "    In order the anova, ancova and anova with interaction to work properly\n",
    "    \n",
    "    '''\n",
    "    for col_name, order in kargs.items():\n",
    "        df[col_name] = pd.Categorical(df[col_name],\n",
    "                                      ordered=True,\n",
    "                                      categories=flatten(order))\n",
    "\n",
    "\n",
    "def day_and_night(df, datetime_column='Date_Time_1', start=7, end=19):\n",
    "    df = df.assign(time=lambda x: np.where(\n",
    "        df[datetime_column].dt.hour.ge(start)\n",
    "        & df[datetime_column].dt.hour.lt(end), 'Day', 'Night')).dropna()\n",
    "    return df\n",
    "\n",
    "\n",
    "def incal_make_averages_table(df,\n",
    "                              columns_names_too_groupby=['Group', 'subjectID'],\n",
    "                              column_name_for_time_of_day='time'):\n",
    "    full_day = df.groupby(by=columns_names_too_groupby, sort=True,\n",
    "                          dropna=True).mean().reset_index().dropna()\n",
    "    full_day[column_name_for_time_of_day] = 'Full day'\n",
    "    D_and_N_df = day_and_night(df).groupby(\n",
    "        by=[column_name_for_time_of_day, *columns_names_too_groupby],\n",
    "        sort=True,\n",
    "        dropna=True).mean().reset_index().dropna()\n",
    "    return pd.concat([full_day, D_and_N_df])\n",
    "\n",
    "\n",
    "# day and night time this data use for the graph below\n",
    "def make_lists_start_and_end_to_day_night_time(df,\n",
    "                                               datetime64_column='Date_Time_1',\n",
    "                                               start=7,\n",
    "                                               end=19):\n",
    "    array_data_list = df[datetime64_column].unique()\n",
    "    Series_datetime64 = pd.Series(array_data_list, name=datetime64_column)\n",
    "    mask_daylight = Series_datetime64.dt.hour.ge(\n",
    "        start) & Series_datetime64.dt.hour.lt(end)\n",
    "    start_end = []\n",
    "    still_True = False\n",
    "    for i in range(len(Series_datetime64)):\n",
    "        if still_True and mask_daylight.iloc[i]:\n",
    "            start_end.append(Series_datetime64.iloc[i])\n",
    "            still_True = False\n",
    "        elif not still_True and not mask_daylight.iloc[i]:\n",
    "            start_end.append(Series_datetime64.iloc[i])\n",
    "            still_True = True\n",
    "    return start_end\n",
    "\n",
    "\n",
    "# stats\n",
    "anova_features = [\n",
    "    'rq', 'locomotor_activity', 'actual_pedmeters_cumsum',\n",
    "    'actual_allmeters_cumsum'\n",
    "]\n",
    "ancova_and_anova_with_interaction_features = [\n",
    "    'Energy_Balance',\n",
    "    'kcal_hr',\n",
    "    'vo2',\n",
    "    'vco2',\n",
    "    'actual_foodupa',\n",
    "    'actual_waterupa',\n",
    "]\n",
    "\n",
    "\n",
    "def reanem_df_by_with_list_by_index(df, indexed_new_names):\n",
    "    columns_names = df.columns.values.tolist()\n",
    "    new_columns_names = indexed_new_names\n",
    "    zip_lists = zip(columns_names, new_columns_names)\n",
    "    dict_renamed_columns = {\n",
    "        column_name: new_column_name\n",
    "        for column_name, new_column_name in zip_lists\n",
    "    }\n",
    "    return df.rename(columns=dict_renamed_columns)\n",
    "\n",
    "\n",
    "def concat_dfs(list_of_series_dfs):\n",
    "    return pd.concat(list_of_series_dfs, axis=1)\n",
    "\n",
    "\n",
    "def anova_with_interaction(df, metabolic_var, independent, categorical):\n",
    "    return ols(\n",
    "        f'{metabolic_var} ~ {independent} + C({categorical}) + {independent}:C({categorical})',\n",
    "        data=df).fit().pvalues\n",
    "\n",
    "\n",
    "def ancova(df, metabolic_var, independent, categorical):\n",
    "    return ols(f'{metabolic_var} ~ {independent} + C({categorical})',\n",
    "               data=df).fit().pvalues\n",
    "\n",
    "\n",
    "def anova(df, metabolic_var, categorical):\n",
    "    return ols(f'{metabolic_var} ~ C({categorical})', data=df).fit().pvalues\n",
    "\n",
    "\n",
    "def make_pvalues_of_anova_analysis(df, m_vars, cat_var):\n",
    "    return [anova(df, m_var, cat_var) for m_var in m_vars]\n",
    "\n",
    "\n",
    "def make_pvalues_of_ancova_analysis(df, m_vars, independent, cat_var):\n",
    "    return [ancova(df, m_var, independent, cat_var) for m_var in m_vars]\n",
    "\n",
    "\n",
    "def make_pvalues_of_anova_with_interaction_analysis(df, m_vars, independent,\n",
    "                                                    cat_var):\n",
    "    return [\n",
    "        anova_with_interaction(df, m_var, independent, cat_var)\n",
    "        for m_var in m_vars\n",
    "    ]\n",
    "\n",
    "\n",
    "def match_case(case, df, list_of_features, independent, category_col_name):\n",
    "    cases = {\n",
    "        'anova':\n",
    "        make_pvalues_of_anova_analysis(df, list_of_features,\n",
    "                                       category_col_name),\n",
    "        'ancova':\n",
    "        make_pvalues_of_ancova_analysis(df, list_of_features, independent,\n",
    "                                        category_col_name),\n",
    "        'anova_with_interaction':\n",
    "        make_pvalues_of_anova_with_interaction_analysis(\n",
    "            df, list_of_features, independent, category_col_name),\n",
    "    }\n",
    "    return cases[case]\n",
    "\n",
    "\n",
    "def incal_create_pvalues_datafram(case, df, list_of_features, independent,\n",
    "                                  category_col_name):\n",
    "    results_from_anovafunction = match_case(case, df, list_of_features,\n",
    "                                            independent, category_col_name)\n",
    "    pvalues_dfs_concated = concat_dfs(results_from_anovafunction)\n",
    "    return reanem_df_by_with_list_by_index(pvalues_dfs_concated,\n",
    "                                           list_of_features)\n",
    "\n",
    "\n",
    "def create_anovas_table(df):\n",
    "    anova_df = incal_create_pvalues_datafram('anova', df, anova_features,\n",
    "                                             'bodymass', 'Group')\n",
    "    anova_with_interaction_df = incal_create_pvalues_datafram(\n",
    "        'anova_with_interaction', df,\n",
    "        ancova_and_anova_with_interaction_features, 'bodymass', 'Group')\n",
    "    # algoritem that get each non p value in anova with interaction and replace it with anova values and fill nan where is needed\n",
    "    ancova_df = incal_create_pvalues_datafram(\n",
    "        'ancova', df, ancova_and_anova_with_interaction_features, 'bodymass',\n",
    "        'Group')\n",
    "    return concat_dfs([anova_df, anova_with_interaction_df, ancova_df]).T\n",
    "\n",
    "\n",
    "dict_aggrageted_function_for_column = {\n",
    "    'Energy_Balance': 'mean',\n",
    "    'actual_allmeters': 'mean',\n",
    "    'actual_pedmeters': 'mean',\n",
    "    'bodymass': 'mean',\n",
    "    'kcal_hr': 'mean',\n",
    "    'locomotor_activity': 'mean',\n",
    "    'rq': 'mean',\n",
    "    'vco2': 'mean',\n",
    "    'vo2': 'mean',\n",
    "    'vh2o': 'mean',\n",
    "    'xbreak': 'mean',\n",
    "    'ybreak': 'mean',\n",
    "    'actual_foodupa': 'sum',\n",
    "    'actual_waterupa': 'sum',\n",
    "}\n",
    "add_feature_for_agg = {\n",
    "    **dict_aggrageted_function_for_column, 'actual_allmeters_cumsum': 'mean',\n",
    "    'actual_pedmeters_cumsum': 'mean'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfed37c2",
   "metadata": {
    "id": "6247bb01-08bd-4d8f-a91a-bfbca4c7acde"
   },
   "outputs": [],
   "source": [
    "# Step 1 - **Upload data**\n",
    "\n",
    "# run cell to upload the data\n",
    "dataframes = {}\n",
    "is_one_file = False\n",
    "# 'this code block for the jupter colab only'\n",
    "# try:\n",
    "#     from google.colab import files\n",
    "#     import io\n",
    "#     uploaded = files.upload()\n",
    "#     if len(uploaded) > 1:\n",
    "#         for fn in uploaded.keys():\n",
    "#             print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "#                 name=fn, length=len(uploaded[fn])))\n",
    "#             dataframes[fn] = pd.read_csv(io.BytesIO(uploaded[fn]))\n",
    "#             is_one_file = False\n",
    "#     else:\n",
    "#         name = list(uploaded.keys())[0]\n",
    "#         dataframes = pd.read_csv(io.BytesIO(uploaded[name]))\n",
    "# except:\n",
    "#     print('check the if error - file.csv, there more then one table in the sheet?, is table has missing columns?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fbe44d9",
   "metadata": {
    "id": "2c1d0a12-4fb3-424c-971f-faae6abcc719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('Control', [1, 4, 7, 10, 13]), ('Group_2', [3, 5, 9, 12, 16]), ('Group_3', [2, 6, 8, 11, 14, 15])])\n"
     ]
    }
   ],
   "source": [
    "dataframes = {\n",
    "    '3': pd.read_csv(\"csvs/shani_exp/hebrew_2021-07-28_16_33_hebrew16_shani_w2_acdoors_pt1_m_calr.csv\", parse_dates=['Date_Time_1']),\n",
    "    '4': pd.read_csv(\"csvs/shani_exp/hebrew_2021-08-01_13_49_hebrew16_shani_w1_pt2b_m_calr(1).csv\", parse_dates=['Date_Time_1']),\n",
    "    '5': pd.read_csv(\"csvs/shani_exp/hebrew_2021-08-04_11_45_hebrew16_shani_acdoors_w2p1_m_calr.csv\", parse_dates=['Date_Time_1']),\n",
    "    '6': pd.read_csv(\"csvs/all_weeks/hebrew_2021-08-10_16_15_hebrew16_shani_w2p2.1_m_calr.csv\", parse_dates=['Date_Time_1']),\n",
    "    '7': pd.read_csv(\"csvs/all_weeks/hebrew_2021-08-11_16_24_hebrew16_shani_acdoors_w3_m_calr.csv\", parse_dates=['Date_Time_1']),\n",
    "    '8': pd.read_csv(\"csvs/all_weeks/hebrew_2021-08-15_16_24_hebrew16_sahni_acdoors_w3p2_m_calr.csv\", parse_dates=['Date_Time_1']),\n",
    "    '9': pd.read_csv(\"csvs/all_weeks/hebrew_2021-08-19_16_17_hebrew16_shani_acdoors_w4_m_calr.csv\", parse_dates=['Date_Time_1']),\n",
    "    '10': pd.read_csv(\"csvs/all_weeks/hebrew_2021-08-26_16_12_hebrew16_shani_acdoors_w5_m_calr.csv\", parse_dates=['Date_Time_1']),\n",
    "    # '11': pd.read_csv(\"csvs/all_weeks/hebrew_2021-08-29_08_41_hebrew16_shani_acdoors_w5_dd_m_calr.csv\", parse_dates=['Date_Time_1']),\n",
    "    # '12': pd.read_csv(\"csvs/all_weeks/hebrew_2021-09-02_07_54_hebrew16_dark dark week2_m_calr.csv\", parse_dates=['Date_Time_1'])\n",
    "}\n",
    "# dataframes = pd.read_csv('csvs\\shani_exp\\hebrew_2021-07-28_16_33_hebrew16_shani_w2_acdoors_pt1_m_calr.csv', parse_dates=['Date_Time_1'])\n",
    "dataframes\n",
    "\n",
    "# Step 2 - **INSERT EXPERIMENT DATA**\n",
    "# expriment design\n",
    "dict_groups = OrderedDict(Control = [1, 4, 7, 10, 13], Group_2 = [3, 5, 9, 12, 16], Group_3 = [2, 6, 8, 11, 14, 15])\n",
    "print(f\"{dict_groups}\")\n",
    "\n",
    "## **Make analysis table**\n",
    "# veriables \n",
    "\n",
    "name_for_replacement_in_data_table = {\n",
    "    'bodymass': 'Weight_(gr)', \n",
    "    'vo2': 'Oxygen_Consumption_(ml/hr)', \n",
    "    'vco2': 'Carbon_Dioxide_Production_(ml/hr)',\n",
    "    'kcal_hr': 'Energy_Expenditure_(kcal/hour)',\n",
    "    'foodupa': 'Cumulative_Food_Intake_(kcal)',\n",
    "    'rq': 'Respiratory_Exchange_Ratio',\n",
    "    'pedmeters': 'Pedestrian_Locomotion_(m)',\n",
    "    'allmeters': 'Total_Distance_includes_fine_movement_(m)',\n",
    "    'waterupa': 'Cumulative_Water_Intake_(ml)',\n",
    "    'Wheel': 'Total_Wheel_Counts_(Counts)' # need to check about the wheel parme...\n",
    "}\n",
    "\n",
    "calculeted_parmeters = {\n",
    "    'water': 'Hourly_Water_Intake_(ml)',   \n",
    "    'food': 'Hourly_Food_Intake_(kcal)',\n",
    "    'locomotor_activity': 'Locomotor_Activity_(beam_breaks)',\n",
    "    'energy_balance': 'Energy_Balance_(kcal/hour)',\n",
    "}\n",
    "\n",
    "unwanted_column = \"|\".join(['envirolightlux', 'envirooccupancy', 'envirorh', 'envirosound', 'envirotemp'])\n",
    "cumulative_parm = \"|\".join(['food', 'water', 'allmeters', 'wheelmeters', 'pedmeters'])\n",
    "not_for_use_columns = ['vh2o', 'xbreak', 'ybreak', 'index'] # do nothig with it becouse it not importent to delete now this columns\n",
    "pattern_addition_to_parms = 'actual_'\n",
    "regx_pattern_for_mean = 'vo2|vco2|vh2o|rq_|bodymass|rq|kcal_hr|break_'\n",
    "regx_pattern_for_sum = 'water|food'\n",
    "\n",
    "dict_aggrageted_function_for_column = {\n",
    "  'Energy_Balance': 'mean',\n",
    "  'actual_allmeters': 'mean',\n",
    "  'actual_pedmeters': 'mean',\n",
    "  'bodymass': 'mean',\n",
    "  'kcal_hr': 'mean',\n",
    "  'locomotor_activity': 'mean',\n",
    "  'rq': 'mean',\n",
    "  'vco2': 'mean',\n",
    "  'vo2': 'mean',\n",
    "  'vh2o': 'mean',\n",
    "  'xbreak': 'mean',\n",
    "  'ybreak': 'mean',\n",
    "  'actual_foodupa': 'sum', \n",
    "  'actual_waterupa': 'sum',\n",
    "}\n",
    "\n",
    "regx_pattern_no_mean_or_sum = regx_pattern_for_sum + regx_pattern_for_mean\n",
    "\n",
    "# Merge files and from experiment file to analysis foramat file\n",
    "df_or_dfs_in_list = [dataframes] if is_one_file else dataframes.values()\n",
    "dfs = [incal_get_actuals_from_cumuletive(df, cumulative_parm, pattern_addition_to_parms) for df in df_or_dfs_in_list]\n",
    "dfs_concated = pd.concat(dfs)\n",
    "\n",
    "dfs_concated_cleaned = select_columns_by_metebolic_parm(dfs_concated, unwanted_column, True) \n",
    "\n",
    "dfs_concated = incal_set_multindex(dfs_concated, ['Date_Time_1']).drop(columns='index')\n",
    "analysis_format = incal_wide_to_long_df(dfs_concated)\n",
    "\n",
    "analysis_format[['vco2', 'vh2o', 'vo2']] = analysis_format[['vco2', 'vh2o', 'vo2']].mul(60)\n",
    "analysis_format[['actual_foodupa']] = analysis_format[['actual_foodupa']].mul(3.56)\n",
    "analysis_format['Energy_Balance'] = analysis_format['actual_foodupa'].values - analysis_format['kcal_hr'].values\n",
    "analysis_format['locomotor_activity'] = analysis_format[['xbreak', 'ybreak']].sum(axis=1)\n",
    "\n",
    "resampled_analysis_format = incal_resample(analysis_format, 'H', dict_aggrageted_function_for_column)\n",
    "\n",
    "featuers_to_cumsum_by = ['actual_pedmeters', 'actual_allmeters']\n",
    "analysis_format = incal_cumsum(analysis_format, ['Date_Time_1', 'subjectID'], featuers_to_cumsum_by)\n",
    "resampled_analysis_format = incal_cumsum(resampled_analysis_format, ['Date_Time_1', 'subjectID'], featuers_to_cumsum_by)\n",
    "\n",
    "add_feature_for_agg = {\n",
    "  **dict_aggrageted_function_for_column, \n",
    "  'actual_allmeters_cumsum': 'mean',\n",
    "  'actual_pedmeters_cumsum': 'mean'\n",
    "}\n",
    "# analysis format - with original datetime samples\n",
    "# when waring with datafram less then 15 K rows we can try to use analysis format\n",
    "\n",
    "# analysis format - datetime agg rolling mean\n",
    "resampled_analysis_format = incal_assemble_multi_index_format(resampled_analysis_format, 'subjectID', dict_groups, 'Group')\n",
    "# analysis format - averages for each subject\n",
    "grouped_analysis_format_df = resampled_analysis_format.groupby(level=['subjectID', 'Group'])\n",
    "analysis_format_calculeted = grouped_analysis_format_df.agg(add_feature_for_agg).dropna()\n",
    "# TODO: need to understend why it not average \n",
    "analysis_format_calculeted['Energy_Balance'] = analysis_format_calculeted['actual_foodupa'].values - analysis_format_calculeted['kcal_hr'].values\n",
    "\n",
    "# **Dashboard**\n",
    "# dashboard code. \n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "app = JupyterDash(__name__, external_stylesheets = external_stylesheets)\n",
    "\n",
    "# !!!!!!!!!!!! format if the file is imported \n",
    "\n",
    "# import design study - expriment data, subject and groups\n",
    "# df = pd.read_csv('csvs/data.csv')  \n",
    "# dict_groups = OrderedDict(Control=[1, 4, 7, 10, 13],\n",
    "#                           Group_2=[3, 5, 9, 12, 16],\n",
    "#                           Group_3=[2, 6, 8, 11, 14, 15])\n",
    "\n",
    "# assemble incal dataframe shape and properties - multiindex\n",
    "# df = incal_create_df_incal_format(df, dict_groups)\n",
    "\n",
    "# !!!!!!!!!!!\n",
    "df = resampled_analysis_format\n",
    "df_removed_outliers = remove_outliers_mixed_df(df)\n",
    "\n",
    "# for layout\n",
    "features = df.columns.values.tolist()\n",
    "subjects_ids = df.index.get_level_values(1)\n",
    "legand_color_order = np.sort([str(n) for n in subjects_ids.unique().values])\n",
    "# select group or subject (category name)\n",
    "categories_columns_names = get_categories_cals_names(df)\n",
    "obj_categories_columns_names = [{\n",
    "    'label': feature,\n",
    "    'value': feature\n",
    "} for feature in categories_columns_names]\n",
    "\n",
    "# trim data - range slider\n",
    "time_stamps = df.index.get_level_values(0)\n",
    "shape_analysis_format_indexed = df.shape\n",
    "end_point_index_analysis_format_indexed = shape_analysis_format_indexed[0] - 1\n",
    "marks_indexed_time_stamp = {\n",
    "    i: time_stamps[i]\n",
    "    for i in range(shape_analysis_format_indexed[0])\n",
    "}\n",
    "# data for Dropdown - removing group or subjects\n",
    "subjects = df.index.get_level_values(1).unique()\n",
    "groups = df.index.get_level_values(2).unique()\n",
    "multi_selection_subjects = [{\n",
    "    'label': str(subject),\n",
    "    'value': str(subject)\n",
    "} for subject in subjects]\n",
    "multi_selection_groups = [{\n",
    "    'label': str(group),\n",
    "    'value': str(group)\n",
    "} for group in groups]\n",
    "\n",
    "# to menage dashboard\n",
    "storage_points_save = {}\n",
    "\n",
    "# levels_as_ids = data.index\n",
    "# levels_ids, levels_uniques = levels_as_ids.factorize()\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.Div([\n",
    "        html.Div(id='father', children=[]),\n",
    "        html.Div([\n",
    "            dcc.Dropdown(id='feature_y_axis_dropdown',\n",
    "                         options=[{\n",
    "                             'label': i,\n",
    "                             'value': i\n",
    "                         } for i in features],\n",
    "                         value=features[0]),\n",
    "            dcc.Dropdown(id='show_as_group_or_individual',\n",
    "                         options=obj_categories_columns_names,\n",
    "                         value=categories_columns_names[0]),\n",
    "            dcc.Dropdown(id='remove_group',\n",
    "                         options=multi_selection_groups,\n",
    "                         multi=True),\n",
    "            dcc.Dropdown(id='remove_subjects',\n",
    "                         options=multi_selection_subjects,\n",
    "                         multi=True),\n",
    "            dcc.Checklist(id='checklist_outliears',\n",
    "                          options=[\n",
    "                              {\n",
    "                                  'label': 'Remove outliers',\n",
    "                                  'value': 'True'\n",
    "                              },\n",
    "                          ],\n",
    "                          value=[],\n",
    "                          labelStyle={'display': 'inline-block'}),\n",
    "            dcc.RangeSlider(id=\"range_slider_trim_time_series\",\n",
    "                            marks=marks_indexed_time_stamp,\n",
    "                            value=(0, end_point_index_analysis_format_indexed),\n",
    "                            allowCross=False,\n",
    "                            min=0,\n",
    "                            max=end_point_index_analysis_format_indexed),\n",
    "        ]),\n",
    "    ]),\n",
    "    html.Div([\n",
    "        dcc.Graph(id='scatter_time_series', clickData={}),\n",
    "        dcc.Graph(id='averages'),\n",
    "        dcc.Graph(id='box', clickData={}),\n",
    "        dcc.Graph(id='hist'),\n",
    "        dcc.Graph(id='regression', clickData={}),\n",
    "        dash_table.DataTable(id='stats_table_Pvalue')\n",
    "    ]),\n",
    "    html.Div([])\n",
    "])\n",
    "\n",
    "\n",
    "def remove_data_points(data):\n",
    "    # remove where keys feature is place\n",
    "    for feature in storage_points_save.keys():\n",
    "        for row_index in storage_points_save[feature]:\n",
    "            data.at[row_index, feature] = np.nan\n",
    "\n",
    "\n",
    "def get_dff(df, v_feature, is_removed_points=False, **kwargs):\n",
    "    dff = df.copy()\n",
    "    return dff[v_feature]\n",
    "\n",
    "\n",
    "def click_data_points(click_data, feature, children):\n",
    "    print(click_data)\n",
    "    point_info = click_data['points'][0]\n",
    "    x_datetime = pd.Timestamp(point_info['x'])\n",
    "    index_legand = point_info['curveNumber']\n",
    "    subject_number = int(legand_color_order[index_legand])\n",
    "    group = [\n",
    "        item[0] for item in list(dict_groups.items())\n",
    "        if subject_number in item[1]\n",
    "    ][0]\n",
    "    row_index = x_datetime, subject_number, group\n",
    "    point = str(row_index)\n",
    "    is_found_feature = feature in storage_points_save.keys()\n",
    "    # get data ids and value after delete point\n",
    "    # get data\n",
    "    if is_found_feature:\n",
    "        is_new_point = point not in storage_points_save.get(feature)\n",
    "        if is_new_point:\n",
    "            storage_points_save[feature].append(row_index)\n",
    "            for item in children:\n",
    "                if item['props']['id']['index'] == feature:\n",
    "                    item['props']['options'].append({\n",
    "                        'label': str(point),\n",
    "                        'value': str(point)\n",
    "                    })\n",
    "    else:\n",
    "        # the user add new feature that dosnt exist in the storage_points_save\n",
    "        storage_points_save[feature] = [row_index]\n",
    "        # when the user adds a feature it then creating a new pattern muching dropdown\n",
    "        new_dropdown = dcc.Dropdown(id={\n",
    "            'type': 'removabal_dropdown',\n",
    "            'index': feature\n",
    "        },\n",
    "                                    options=[{\n",
    "                                        'label': str(i),\n",
    "                                        'value': str(i)\n",
    "                                    } for i in storage_points_save[feature]],\n",
    "                                    multi=True)\n",
    "        children.append(new_dropdown)\n",
    "    return children\n",
    "\n",
    "\n",
    "def create_scatter(dff, colors):\n",
    "    x_axis = dff.index.get_level_values(0)\n",
    "    color = dff.index.get_level_values(1)\n",
    "    y_axis = dff.values\n",
    "\n",
    "    fig = px.scatter(x=x_axis,\n",
    "                     y=y_axis,\n",
    "                     color=color,\n",
    "                     color_discrete_sequence=colors,\n",
    "                     template='simple_white')\n",
    "    fig.update_traces(mode='lines+markers')\n",
    "    fig.update_layout(legend_traceorder=\"normal\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_bar(averages, colors):\n",
    "    groups = averages.index.get_level_values(1)\n",
    "    y_axis = averages.values\n",
    "    return px.bar(x=groups,\n",
    "                  y=y_axis,\n",
    "                  color=groups,\n",
    "                  color_discrete_sequence=colors,\n",
    "                  template='simple_white')\n",
    "\n",
    "\n",
    "def create_histogram(time_series, category_name, colors):\n",
    "    x_axis = time_series.values\n",
    "    color_group = time_series.index.get_level_values(category_name).values\n",
    "    return px.histogram(x=x_axis,\n",
    "                        color=color_group,\n",
    "                        color_discrete_sequence=colors,\n",
    "                        template='simple_white')\n",
    "\n",
    "\n",
    "def create_regression(averages_df, colors, feature_name):\n",
    "    group_color = averages_df.index.get_level_values(1)\n",
    "    x_axis = averages_df['bodymass'].values\n",
    "    feature_name = feature_name if feature_name != 'bodymass' else 'rq'\n",
    "    y_axis = averages_df[feature_name].values\n",
    "    return px.scatter(x=x_axis,\n",
    "                      y=y_axis,\n",
    "                      color=group_color,\n",
    "                      color_discrete_sequence=colors,\n",
    "                      template='simple_white',\n",
    "                      trendline='ols')\n",
    "\n",
    "\n",
    "def create_box(time_series, category_name, colors):\n",
    "    x_axis = time_series.index.get_level_values(category_name).values\n",
    "    y_axis = time_series.values\n",
    "    return px.box(x=x_axis,\n",
    "                  y=y_axis,\n",
    "                  color=x_axis,\n",
    "                  color_discrete_sequence=colors,\n",
    "                  template='simple_white')\n",
    "\n",
    "\n",
    "def removing_group_or_subjects(data, remove_group, remove_subjects):\n",
    "    if remove_group:\n",
    "        return incal_remove_group(data, 2, remove_group)\n",
    "    elif remove_subjects:\n",
    "        return incal_remove_subjects(data, 1, remove_subjects)\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_average_df(data, features_calc):\n",
    "    # averages df\n",
    "    subjects_ids = data.index.get_level_values('subjectID')\n",
    "    groups_ids = data.index.get_level_values('Group')\n",
    "    return data.groupby([subjects_ids, groups_ids]).agg(features_calc).dropna(\n",
    "    )  # dropna to get rid from the 0 and nan where groupby calc on subject that dosn't belong to group\n",
    "\n",
    "\n",
    "def groupby_category(data, category, features_calc):\n",
    "    if category == 'subjectID':\n",
    "        return data\n",
    "    datetime = data.index.get_level_values('Date_Time_1')\n",
    "    groups = data.index.get_level_values(category)\n",
    "    grouped_data = data.groupby([datetime, groups])\n",
    "    return grouped_data.agg(features_calc).dropna()\n",
    "\n",
    "\n",
    "def statstical_analysis(averages_df):\n",
    "    grouped_analysis_format_df = averages_df\n",
    "    analysis_format_calculeted_index_reseted = \\\n",
    "        grouped_analysis_format_df.reset_index()\n",
    "    p_values_table = create_anovas_table(\n",
    "        analysis_format_calculeted_index_reseted)\n",
    "    p_values_table = p_values_table.reset_index().rename(\n",
    "        columns={'index': 'Features'})\n",
    "    columns = [{'id': p, 'name': p} for p in p_values_table.columns.to_list()]\n",
    "    table = p_values_table.to_dict('records')\n",
    "    return columns, table\n",
    "\n",
    "\n",
    "@dash.callback(\n",
    "    Output('scatter_time_series', 'figure'),\n",
    "    Output('averages', 'figure'),\n",
    "    Output('box', 'figure'),\n",
    "    Output('hist', 'figure'),\n",
    "    Output('regression', 'figure'),\n",
    "    Output('father', 'children'),\n",
    "    Output('stats_table_Pvalue', 'columns'),\n",
    "    Output('stats_table_Pvalue', 'data'),\n",
    "    Input('feature_y_axis_dropdown', 'value'),\n",
    "    Input('remove_subjects', 'value'),\n",
    "    Input('remove_group', 'value'),\n",
    "    Input('checklist_outliears', 'value'),\n",
    "    Input('range_slider_trim_time_series', 'value'),\n",
    "    Input('range_slider_trim_time_series', 'marks'),\n",
    "    Input('scatter_time_series', 'clickData'),\n",
    "    Input('show_as_group_or_individual', 'value'),  # it is there to call the \n",
    "    State('show_as_group_or_individual', 'value'),\n",
    "    State('feature_y_axis_dropdown',\n",
    "          'value'),  # getting current feature name from dropdown\n",
    "    State('father', 'children'))\n",
    "def pool_dashboard_data(value_feature, remove_subjects, remove_group,\n",
    "                        checklist_outliers, tuple_start_end, dict_time_stamps,\n",
    "                        click_data, input_category, category_name,\n",
    "                        state_feature, children):\n",
    "\n",
    "    info = dash.callback_context\n",
    "    is_clickData_triggered = info.triggered[0][\n",
    "        'prop_id'] == 'scatter_time_series.clickData'\n",
    "    # remove outliers\n",
    "    data = df.copy() if not checklist_outliers else df_removed_outliers.copy()\n",
    "    # remove specific points\n",
    "    print(30)\n",
    "    if is_clickData_triggered or (state_feature in storage_points_save.keys()\n",
    "                                  ):  # removing data points that been click\n",
    "        children = click_data_points(click_data, state_feature, children)\n",
    "        remove_data_points(data)  # inplace\n",
    "\n",
    "    # trim datetime from the sides\n",
    "    start_time, end_time = get_start_and_end_time(tuple_start_end,\n",
    "                                                  dict_time_stamps)\n",
    "    data = trim_df_datetime(data, start_time, end_time)\n",
    "\n",
    "    # removing group or subjects depnding on the selection\n",
    "    data = removing_group_or_subjects(data, remove_group, remove_subjects)\n",
    "\n",
    "    # selecting and grouping data\n",
    "    features_calc = add_feature_for_agg  # dict - key (column_name): value (calc for parmeter) - this dict is for aggregetion function for each feature\n",
    "    # creating an averages df\n",
    "    averages_df = create_average_df(data, features_calc)\n",
    "    averages_df['Energy_Balance'] = averages_df[\n",
    "        'actual_foodupa'].values - averages_df['kcal_hr'].values\n",
    "    # timeseries - groupby subject or groups\n",
    "    time_series_df = groupby_category(data, category_name, features_calc)\n",
    "    # selecting column by \"state_feature\" (feature state is all the parmeters of the data i.e: Energy_Balance)\n",
    "    # selecting for each \"_df\" (averages_df, time_series_df)\n",
    "    time_series = get_dff(time_series_df, state_feature)\n",
    "    averages = get_dff(averages_df, state_feature)\n",
    "\n",
    "    colors = px.colors.qualitative.Vivid\n",
    "    fig_scatter = create_scatter(time_series, colors)\n",
    "    fig_bar = create_bar(averages, colors)\n",
    "    fig_box = create_box(time_series, category_name, colors)\n",
    "    fig_histogram = create_histogram(time_series, category_name, colors)\n",
    "    fig_regression = create_regression(averages_df, colors, state_feature)\n",
    "\n",
    "    # analysis section\n",
    "    columns, table = statstical_analysis(averages_df)\n",
    "    return fig_scatter, fig_bar, fig_box, fig_histogram, fig_regression, children, columns, table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1974cdcf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "N6R7FmDICrp-",
    "outputId": "305af7a0-2c6b-4282-8bba-e4a47819dd18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:8024/\n"
     ]
    }
   ],
   "source": [
    "def run_server(self,\n",
    "               port=8004,\n",
    "               debug=True,\n",
    "               threaded=True,\n",
    "               **flask_run_options):\n",
    "    self.server.run(port=port, debug=debug, **flask_run_options)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True, port=8024, mode='external')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b033b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://127.0.0.1:8024/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "292942d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2021-07-28 16:00:00', '2021-07-28 16:00:00',\n",
       "               '2021-07-28 16:00:00', '2021-07-28 16:00:00',\n",
       "               '2021-07-28 16:00:00', '2021-07-28 16:00:00',\n",
       "               '2021-07-28 16:00:00', '2021-07-28 16:00:00',\n",
       "               '2021-07-28 16:00:00', '2021-07-28 16:00:00',\n",
       "               ...\n",
       "               '2021-08-29 08:00:00', '2021-08-29 08:00:00',\n",
       "               '2021-08-29 08:00:00', '2021-08-29 08:00:00',\n",
       "               '2021-08-29 08:00:00', '2021-08-29 08:00:00',\n",
       "               '2021-08-29 08:00:00', '2021-08-29 08:00:00',\n",
       "               '2021-08-29 08:00:00', '2021-08-29 08:00:00'],\n",
       "              dtype='datetime64[ns]', name='Date_Time_1', length=11408, freq=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = resampled_analysis_format.index.get_level_values\n",
    "x(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe1dcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalIndex(['1', '10', '11', '12', '13', '14', '15', '16', '2', '3',\n",
       "                  ...\n",
       "                  '15', '16', '2', '3', '4', '5', '6', '7', '8', '9'],\n",
       "                 categories=['1', '4', '7', '10', '13', '3', '5', '9', ...], ordered=True, dtype='category', name='subjectID', length=11408)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91c9c1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalIndex(['Control', 'Control', 'Group_3', 'Group_2', 'Control',\n",
       "                  'Group_3', 'Group_3', 'Group_2', 'Group_3', 'Group_2',\n",
       "                  ...\n",
       "                  'Group_3', 'Group_2', 'Group_3', 'Group_2', 'Control',\n",
       "                  'Group_2', 'Group_3', 'Control', 'Group_3', 'Group_2'],\n",
       "                 categories=['Control', 'Group_2', 'Group_3'], ordered=True, dtype='category', name='Group', length=11408)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12fcd19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of in-cal-nootbook.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "66599a7a6142eae9f63457fbd787ef4b55802012b5e3fbb704210ee6f3bbe423"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
